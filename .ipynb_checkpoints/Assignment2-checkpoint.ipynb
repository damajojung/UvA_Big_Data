{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6010828a",
   "metadata": {},
   "source": [
    "### Before you begin\n",
    "\n",
    "To check whether the code you've written is correct, we'll again use automark. Please update the username in the cell below with your student number. \n",
    "\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "In the Spark tasks in this assignment, Spark might randomly crash if it doesn't have enough memory available. If you encounter issues like that when working on the tasks today, we suggest increasing the amount of memory available to the docker image to 4gb or more. If you are using docker-for-windows or docker-for-mac, you can easily increase it from the Whale ðŸ³ icon in the task bar, then go to Preferences -> Resources.\n",
    "\n",
    "Note that passing a command line argument like `-m 4g` when running `docker run ...` does only work for limiting the memory to even lower values than the limit in your Docker Resource Preferences, and does not work for relaxing the limit that is set in these Preferences. For more information, please refer to the Docker [documentation](https://docs.docker.com/config/containers/resource_constraints/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "66585a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "| Jung, David                                                        |\n",
      "----------------------------------------------------------------------\n",
      "| a0_t1_sailor_avg_experience                       | completed      |\n",
      "| a0_t2_sailor_avg_experience_pyspark               | completed      |\n",
      "| a1a_t1_all_model_number_b                         | completed      |\n",
      "| a1a_t2_all_model_number_not_b                     | completed      |\n",
      "| a1a_t3_avg_PCs_speed                              | completed      |\n",
      "| a1a_t4_max_printer_price                          | completed      |\n",
      "| a1a_t5_min_spending_visa                          | completed      |\n",
      "| a1a_t6_customer_id                                | completed      |\n",
      "| a1a_t7_highest_price_USD                          | completed      |\n",
      "| a1b_t1a_at_least_four_products                    | completed      |\n",
      "| a1b_t1b_decending_speed_laptop                    | completed      |\n",
      "| a1b_t1c_customer_number_purchase                  | completed      |\n",
      "| a1b_t1d_maker_with_ram                            | completed      |\n",
      "| a1b_t2a_avg_money_spent                           | completed      |\n",
      "| a1b_t2b_maximum_price_printer                     | completed      |\n",
      "| a2_t1a_max_measurement_per_sensor                 | completed      |\n",
      "| a2_t1b_global_max_measurement                     | completed      |\n",
      "| a2_t2_commacount                                  | completed      |\n",
      "| a2_t3_max_weekly_visitors                         | completed      |\n",
      "| a2_t4a_compute_max_value_per_sensor               | not attempted  |\n",
      "| a2_t4b_compute_global_max_value                   | not attempted  |\n",
      "| a2_t5_compute_max_weekly_visitors                 | not attempted  |\n",
      "| a2_t6_customers_with_certain_visa_payment         | not attempted  |\n",
      "| a2_t7_find_payment_types                          | not attempted  |\n",
      "| a2_t8_quantities_per_city                         | not attempted  |\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import automark as am\n",
    "import math\n",
    "\n",
    "# fill in you student number as your username\n",
    "am.configure(username='13646168')\n",
    "\n",
    "# to check your progress, you can run this function\n",
    "am.get_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4bab27",
   "metadata": {},
   "source": [
    "# Part A - MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d1e374",
   "metadata": {},
   "source": [
    "## Our MapReduce Engine\n",
    "\n",
    "Real-world MapReduce systems like [Apache Hadoop](https://hadoop.apache.org/) are tedious to install and debug, and require code in a programming language like Java. For the sake of simplicity and to allow for easy debugging, we work with our own local, non-parallel MapReduce engine written in Python. Note that your MapReduce programs (if written correctly) would also work in distributed mode on a real cluster.\n",
    "\n",
    "In the following, we ask you to implement several mapreduce programs and run them via this simple MapReduce engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1fbd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapreduce(input_partitions, f_map, f_reduce, num_reducers=2, print_debug_text=False):\n",
    "    if print_debug_text: print(\"---Starting MAP-phase.---\")\n",
    "    # These arrays will hold the outputs of the map-phase for each input partition\n",
    "    map_output_partitions = []\n",
    "    # We are running the map-phase on each input partition now. In a real mapreduce system, this would run\n",
    "    # in parallel on different machines.\n",
    "    for counter, input_partition in enumerate(input_partitions):\n",
    "        if print_debug_text: print(f\"  Applying f_map to input partition {counter}\")\n",
    "        map_output = []\n",
    "        # We apply f_map to each (key, value) pair in the input partition, and store the corresponding outputs\n",
    "        for key, value in input_partition:\n",
    "            if print_debug_text: print(f\"    f_map({key}, {value}) -> \")\n",
    "            for mapped_key, mapped_value in f_map(key, value):\n",
    "                if print_debug_text: print(f\"      ({mapped_key}, {mapped_value})\")\n",
    "                map_output.append((mapped_key, mapped_value))\n",
    "        # Store output partition of this map operation        \n",
    "        map_output_partitions.append(map_output)        \n",
    "    \n",
    "    # Next, we start the shuffle phase. We need to create several reducer partitions from the map outputs, \n",
    "    # assign each key to a partition and collect all the values for this key.\n",
    "    if print_debug_text: print(\"\\n---Starting SHUFFLE-phase.---\")\n",
    "    reduce_input_partitions = []\n",
    "    # We create as many reducer partitions as specified by num_reducers\n",
    "    for _ in range(0, num_reducers):\n",
    "        reduce_input_partitions.append(dict())\n",
    "        \n",
    "    # We shuffle each map output partition now. In a real mapreduce system, this would run\n",
    "    # in parallel on different machines.     \n",
    "    for counter, map_output_partition in enumerate(map_output_partitions):\n",
    "        if print_debug_text: print(f\"  Shuffling map output input partition {counter}\")    \n",
    "        # We process each (key, value) pair from the map-output here\n",
    "        for key, value in map_output_partition:\n",
    "            # We determine the target partition for this key via hash-partitioning\n",
    "            target_partition_index = abs(hash(key)) % num_reducers\n",
    "            if print_debug_text: print(f\"    Assigning key [{key}] to reducer input {target_partition_index} via hash-partitioning\")            \n",
    "            # We add the value to the group of the key in the target partition\n",
    "            target_partition = reduce_input_partitions[target_partition_index]\n",
    "            if not key in target_partition:\n",
    "                target_partition[key] = []            \n",
    "            target_partition[key].append(value)\n",
    "    \n",
    "    # Next, we run the reduce-phase\n",
    "    if print_debug_text: print(\"\\n---Starting REDUCE-phase.---\")\n",
    "    reduce_output_partitions = []\n",
    "        \n",
    "    # We reduce each reduce partition now. In a real mapreduce system, this would run\n",
    "    # in parallel on different machines.            \n",
    "    for counter, reduce_input_partition in enumerate(reduce_input_partitions):\n",
    "        reduce_output = []\n",
    "        if print_debug_text: print(f\"  Applying f_reduce to reduce_input partition {counter}\")    \n",
    "        # We apply f_reduce to each key and its corresponding values now and collect the results\n",
    "        for key, values in reduce_input_partition.items():\n",
    "            if print_debug_text: print(f\"    f_reduce({key}, {values}) -> \")\n",
    "            for reduced_key, reduced_value in f_reduce(key, values):\n",
    "                if print_debug_text: print(f\"      ({reduced_key}, {reduced_value})\")\n",
    "                reduce_output.append((reduced_key, reduced_value))\n",
    "        reduce_output_partitions.append(reduce_output)\n",
    "        \n",
    "    return reduce_output_partitions     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378f50d",
   "metadata": {},
   "source": [
    "The following is a helper function for local testing, which checks the size of the result of a mapreduce job and tests where certain keys are present and have the expected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5352194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_locally(mapreduce_result, expected_num_keys, expected_keys_and_values):\n",
    "    print(\"\\nRESULT VERIFICATION:\")\n",
    "    results_by_key = {}\n",
    "    for partition in mapreduce_result:\n",
    "        for key, value in partition:\n",
    "            results_by_key[key] = value\n",
    "\n",
    "    if len(results_by_key) != expected_num_keys:\n",
    "        print(f\"  ERROR: Expected {len(expected_keys_and_values)} keys in the output, but found {len(results_by_key)}!\")\n",
    "\n",
    "    for key_of_interest, expected_value in expected_keys_and_values:\n",
    "        if key_of_interest not in results_by_key:\n",
    "            print(f\"  ERROR: Unable to find key [{key_of_interest}] in result!\")\n",
    "        else:\n",
    "            observed_value = results_by_key[key_of_interest]\n",
    "            if isinstance(expected_value, (float, int)):\n",
    "                if not math.isclose(expected_value, observed_value, rel_tol=1e-5, abs_tol=1e-5):\n",
    "                    print(f\"  ERROR: Key [{key_of_interest}] has unexpected value [{observed_value}]!\")\n",
    "                else:\n",
    "                    print(f\"  PASS: Key [{key_of_interest}] has expected value!\")\n",
    "            else:\n",
    "                if expected_value is not observed_value:\n",
    "                    print(f\"  ERROR: Key [{key_of_interest}] has unexpected value [{observed_value}]!\")\n",
    "                else:\n",
    "                    print(f\"  PASS: Key [{key_of_interest}] has expected value!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0391cc",
   "metadata": {},
   "source": [
    "## WordCount - the \"Hello World\" of MapReduce\n",
    "\n",
    "Before we ask you to write your own MapReduce program, you should first investigate the example from our lecture. \n",
    "\n",
    "Our input are two partitions of key value pairs denoting documents, where the document is the key, and the value is text of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c92132a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_documents = [\n",
    "    # Input partition 0\n",
    "    [\n",
    "        (1, \"the shawshank redemption\"),\n",
    "        (2, \"the godfather\")\n",
    "    ],\n",
    "    # Input partition 1\n",
    "    [\n",
    "        (3, \"the godfather part two\")\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0000be81",
   "metadata": {},
   "source": [
    "We want to count the number of occurrences of each word using mapreduce. The following code shows the corresponding functions `f_map` and `f_reduce`.  The `f_map` function split the text into words and emits a key value pair `(word, 1)` for each word it finds. The `f_reduce` function receives a word as key and its associated counts, and computes the total number of occurrences from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "172dd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount_f_map(doc_id, text):   \n",
    "    output_tuples = []\n",
    "    for word in text.split(\" \"):\n",
    "        output_tuples.append((word, 1))\n",
    "    return output_tuples        \n",
    "\n",
    "def wordcount_f_reduce(word, counts):\n",
    "    total_count = 0\n",
    "    for count in counts:\n",
    "        total_count += count\n",
    "    return [(word, total_count)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960b810",
   "metadata": {},
   "source": [
    "Next, we will execute the program in the next cell. The following diagram shows the flow of data during the execution of our MapReduce program. After execution, look at the log outputs of our engine and compare them to the diagram. Can you see the correspondence?\n",
    "\n",
    "![title](wordcount.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a79a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting MAP-phase.---\n",
      "  Applying f_map to input partition 0\n",
      "    f_map(1, the shawshank redemption) -> \n",
      "      (the, 1)\n",
      "      (shawshank, 1)\n",
      "      (redemption, 1)\n",
      "    f_map(2, the godfather) -> \n",
      "      (the, 1)\n",
      "      (godfather, 1)\n",
      "  Applying f_map to input partition 1\n",
      "    f_map(3, the godfather part two) -> \n",
      "      (the, 1)\n",
      "      (godfather, 1)\n",
      "      (part, 1)\n",
      "      (two, 1)\n",
      "\n",
      "---Starting SHUFFLE-phase.---\n",
      "  Shuffling map output input partition 0\n",
      "    Assigning key [the] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [shawshank] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [redemption] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [the] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [godfather] to reducer input 1 via hash-partitioning\n",
      "  Shuffling map output input partition 1\n",
      "    Assigning key [the] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [godfather] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [part] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [two] to reducer input 0 via hash-partitioning\n",
      "\n",
      "---Starting REDUCE-phase.---\n",
      "  Applying f_reduce to reduce_input partition 0\n",
      "    f_reduce(shawshank, [1]) -> \n",
      "      (shawshank, 1)\n",
      "    f_reduce(part, [1]) -> \n",
      "      (part, 1)\n",
      "    f_reduce(two, [1]) -> \n",
      "      (two, 1)\n",
      "  Applying f_reduce to reduce_input partition 1\n",
      "    f_reduce(the, [1, 1, 1]) -> \n",
      "      (the, 3)\n",
      "    f_reduce(redemption, [1]) -> \n",
      "      (redemption, 1)\n",
      "    f_reduce(godfather, [1, 1]) -> \n",
      "      (godfather, 2)\n"
     ]
    }
   ],
   "source": [
    "wordcount_result = mapreduce(partitioned_documents, wordcount_f_map, wordcount_f_reduce, print_debug_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a6a9b",
   "metadata": {},
   "source": [
    "Test your program by running the following cell, which checks whether your program produces the correct output for our example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6943c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT VERIFICATION:\n",
      "  PASS: Key [the] has expected value!\n",
      "  PASS: Key [godfather] has expected value!\n",
      "  PASS: Key [two] has expected value!\n"
     ]
    }
   ],
   "source": [
    "verify_locally(wordcount_result, 6, [(\"the\", 3), (\"godfather\", 2), (\"two\", 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594a706",
   "metadata": {},
   "source": [
    "## Task 1: Sensor Measurements\n",
    "\n",
    "In the following, we ask you to write some mapreduce programs of your own. In the first task, we will work with sensor data. We have a couple of __sensors__ that have a name and produce some numerical value as measurement. Our data consists of several partitions, which contain key value pairs with the __name__ of the sensor as key and the __measurement__ as value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "800c618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_sensor_measurements = [\n",
    "    [\n",
    "        (\"Sensor1\", 12.3),\n",
    "        (\"Sensor2\", 9.3),\n",
    "        (\"Sensor3\", 2.7),\n",
    "        (\"BrokenSensor\", 1000.0),\n",
    "    ],\n",
    "    [\n",
    "        (\"Sensor1\", 17.1),\n",
    "        (\"Sensor2\", 7.3),\n",
    "        (\"Sensor3\", -12),\n",
    "    ],\n",
    "    [\n",
    "        (\"Sensor1\", 19.1),\n",
    "        (\"BrokenSensor\", 5000.0),\n",
    "        (\"Sensor2\", 3.3),\n",
    "        (\"Sensor3\", 0.0),\n",
    "    ],    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f8bc23a-6630-433c-93d7-741425b06704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(partitioned_sensor_measurements[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da655fd7-24a9-426d-9a1a-bde2fa4ce60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sensor1', 12.3),\n",
       " ('Sensor2', 9.3),\n",
       " ('Sensor3', 2.7),\n",
       " ('BrokenSensor', 1000.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitioned_sensor_measurements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec9ec6b7-1da3-4491-bd39-313268911e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  ('x', 19.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95d1eaa1-285a-4ffc-bc62-f6deec0d989a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1] == 19.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82f8f9fe-9789-4721-a409-b0cf71a299e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [('Sensor1', 12.3),\n",
    " ('Sensor2', 9.3),\n",
    " ('Sensor3', 2.7),\n",
    " ('BrokenSensor', 1000.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "828c5803-edd7-4c8c-9846-2da57531350b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Sensor1' in test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a875c92c-7ce4-4a0a-a719-08d174aa5000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0\n",
      "b [('Sensor1', 12.3), ('Sensor2', 9.3), ('Sensor3', 2.7), ('BrokenSensor', 1000.0)]\n",
      "c Sensor1\n",
      "d 12.3\n",
      "c Sensor2\n",
      "d 9.3\n",
      "c Sensor3\n",
      "d 2.7\n",
      "a 1\n",
      "b [('Sensor1', 17.1), ('Sensor2', 7.3), ('Sensor3', -12)]\n",
      "c Sensor1\n",
      "d 17.1\n",
      "c Sensor2\n",
      "d 7.3\n",
      "c Sensor3\n",
      "d -12\n",
      "a 2\n",
      "b [('Sensor1', 19.1), ('BrokenSensor', 5000.0), ('Sensor2', 3.3), ('Sensor3', 0.0)]\n",
      "c Sensor1\n",
      "d 19.1\n",
      "c Sensor2\n",
      "d 3.3\n",
      "c Sensor3\n",
      "d 0.0\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for a,b in enumerate(partitioned_sensor_measurements):\n",
    "    print('a', a)\n",
    "    print('b',b)\n",
    "    for c,d in b:\n",
    "        if c != 'BrokenSensor':\n",
    "            print('c', c)\n",
    "            print('d',d)\n",
    "            l.append((c,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cefee489-4818-404a-a759-96afaf5adcfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sensor1', 12.3),\n",
       " ('Sensor2', 9.3),\n",
       " ('Sensor3', 2.7),\n",
       " ('Sensor1', 17.1),\n",
       " ('Sensor2', 7.3),\n",
       " ('Sensor3', -12),\n",
       " ('Sensor1', 19.1),\n",
       " ('Sensor2', 3.3),\n",
       " ('Sensor3', 0.0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91dbcdb8-cf8e-4f10-bdbb-7826c663af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sensor1', 12.3)\n",
      "('Sensor2', 9.3)\n",
      "('Sensor3', 2.7)\n",
      "('Sensor1', 17.1)\n",
      "('Sensor2', 7.3)\n",
      "('Sensor3', -12)\n",
      "('Sensor1', 19.1)\n",
      "('Sensor2', 3.3)\n",
      "('Sensor3', 0.0)\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "for i in l:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a7438daa-ed21-4510-8279-87323739fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = [12.3, 17.1, 19.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b5b79cc-0a24-4d02-9393-68ec6fc18610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(bla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379e957-1101-4eeb-afd4-5271a9bd6fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddd330b",
   "metadata": {},
   "source": [
    "### 1(a) - Compute the maximum measurement per sensor (except for the broken sensor)\n",
    "\n",
    "Time to write your first mapreduce program. Your task is to compute the maximum measurement value per sensor, for all sensors except the \"BrokenSensor\". Implement your code in the `max_measurement_per_sensor_f_map` and `max_measurement_per_sensor_f_reduce` functions (and change the return statement).\n",
    "\n",
    "Please note that the result of your mapreduce program should be equivalent to running the following SQL query on the data (if it was in a database):\n",
    "\n",
    "```\n",
    "SELECT sensor, MAX(measurement)\n",
    "FROM sensors\n",
    "WHERE name != \"BrokenSensor\"\n",
    "GROUP BY sensor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b09d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2_t1a_max_measurement_per_sensor():\n",
    "    def max_measurement_per_sensor_f_map(sensor, measurement): \n",
    "        output = []\n",
    "        if sensor != \"BrokenSensor\":\n",
    "            output.append((sensor, measurement))\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def max_measurement_per_sensor_f_reduce(sensor, measurements):\n",
    "        out = [(sensor, max(measurements))]\n",
    "        return out\n",
    "    \n",
    "    return max_measurement_per_sensor_f_map, max_measurement_per_sensor_f_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2393048a",
   "metadata": {},
   "source": [
    "Execute your program by running the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a701b360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting MAP-phase.---\n",
      "  Applying f_map to input partition 0\n",
      "    f_map(Sensor1, 12.3) -> \n",
      "      (Sensor1, 12.3)\n",
      "    f_map(Sensor2, 9.3) -> \n",
      "      (Sensor2, 9.3)\n",
      "    f_map(Sensor3, 2.7) -> \n",
      "      (Sensor3, 2.7)\n",
      "    f_map(BrokenSensor, 1000.0) -> \n",
      "  Applying f_map to input partition 1\n",
      "    f_map(Sensor1, 17.1) -> \n",
      "      (Sensor1, 17.1)\n",
      "    f_map(Sensor2, 7.3) -> \n",
      "      (Sensor2, 7.3)\n",
      "    f_map(Sensor3, -12) -> \n",
      "      (Sensor3, -12)\n",
      "  Applying f_map to input partition 2\n",
      "    f_map(Sensor1, 19.1) -> \n",
      "      (Sensor1, 19.1)\n",
      "    f_map(BrokenSensor, 5000.0) -> \n",
      "    f_map(Sensor2, 3.3) -> \n",
      "      (Sensor2, 3.3)\n",
      "    f_map(Sensor3, 0.0) -> \n",
      "      (Sensor3, 0.0)\n",
      "\n",
      "---Starting SHUFFLE-phase.---\n",
      "  Shuffling map output input partition 0\n",
      "    Assigning key [Sensor1] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [Sensor2] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [Sensor3] to reducer input 1 via hash-partitioning\n",
      "  Shuffling map output input partition 1\n",
      "    Assigning key [Sensor1] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [Sensor2] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [Sensor3] to reducer input 1 via hash-partitioning\n",
      "  Shuffling map output input partition 2\n",
      "    Assigning key [Sensor1] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [Sensor2] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [Sensor3] to reducer input 1 via hash-partitioning\n",
      "\n",
      "---Starting REDUCE-phase.---\n",
      "  Applying f_reduce to reduce_input partition 0\n",
      "  Applying f_reduce to reduce_input partition 1\n",
      "    f_reduce(Sensor1, [12.3, 17.1, 19.1]) -> \n",
      "      (Sensor1, 19.1)\n",
      "    f_reduce(Sensor2, [9.3, 7.3, 3.3]) -> \n",
      "      (Sensor2, 9.3)\n",
      "    f_reduce(Sensor3, [2.7, -12, 0.0]) -> \n",
      "      (Sensor3, 2.7)\n"
     ]
    }
   ],
   "source": [
    "result = mapreduce(partitioned_sensor_measurements, *a2_t1a_max_measurement_per_sensor(), print_debug_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede05a7",
   "metadata": {},
   "source": [
    "Test your program by running the following cell, which checks whether your program produces the correct output for our example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ab08ca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT VERIFICATION:\n",
      "  PASS: Key [Sensor1] has expected value!\n",
      "  PASS: Key [Sensor2] has expected value!\n",
      "  PASS: Key [Sensor3] has expected value!\n"
     ]
    }
   ],
   "source": [
    "verify_locally(result, 3, [(\"Sensor1\", 19.1), (\"Sensor2\", 9.3), (\"Sensor3\", 2.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8a5d791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local tests...\n",
      "a2_t1a_max_measurement_per_sensor successfully passed local tests\n",
      "Running remote test...\n",
      "Test was successful. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "am.test_student_function(a2_t1a_max_measurement_per_sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2228e16",
   "metadata": {},
   "source": [
    "### 1(b) - Compute the globally maximal measurement\n",
    "\n",
    "In the next task, you have to compute the globally maximal measurement value from all sensors (including the \"BrokenSensor\"). Implement your code in the `global_max_measurement_f_map` and `global_max_measurement_f_reduce` functions (and change the return statement).\n",
    "\n",
    "Hint: use an artificial key \"global\" to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5f3a0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2_t1b_global_max_measurement():\n",
    "    def global_max_measurement_f_map(sensor, measurement):\n",
    "        return [('global', measurement)]\n",
    "    \n",
    "    def global_max_measurement_f_reduce(sensor, measurements):\n",
    "        out = [('global', max(measurements))]\n",
    "        return out\n",
    "    \n",
    "    return global_max_measurement_f_map, global_max_measurement_f_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc44517",
   "metadata": {},
   "source": [
    "Execute your program by running the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a384fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting MAP-phase.---\n",
      "  Applying f_map to input partition 0\n",
      "    f_map(Sensor1, 12.3) -> \n",
      "      (global, 12.3)\n",
      "    f_map(Sensor2, 9.3) -> \n",
      "      (global, 9.3)\n",
      "    f_map(Sensor3, 2.7) -> \n",
      "      (global, 2.7)\n",
      "    f_map(BrokenSensor, 1000.0) -> \n",
      "      (global, 1000.0)\n",
      "  Applying f_map to input partition 1\n",
      "    f_map(Sensor1, 17.1) -> \n",
      "      (global, 17.1)\n",
      "    f_map(Sensor2, 7.3) -> \n",
      "      (global, 7.3)\n",
      "    f_map(Sensor3, -12) -> \n",
      "      (global, -12)\n",
      "  Applying f_map to input partition 2\n",
      "    f_map(Sensor1, 19.1) -> \n",
      "      (global, 19.1)\n",
      "    f_map(BrokenSensor, 5000.0) -> \n",
      "      (global, 5000.0)\n",
      "    f_map(Sensor2, 3.3) -> \n",
      "      (global, 3.3)\n",
      "    f_map(Sensor3, 0.0) -> \n",
      "      (global, 0.0)\n",
      "\n",
      "---Starting SHUFFLE-phase.---\n",
      "  Shuffling map output input partition 0\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "  Shuffling map output input partition 1\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "  Shuffling map output input partition 2\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [global] to reducer input 0 via hash-partitioning\n",
      "\n",
      "---Starting REDUCE-phase.---\n",
      "  Applying f_reduce to reduce_input partition 0\n",
      "    f_reduce(global, [12.3, 9.3, 2.7, 1000.0, 17.1, 7.3, -12, 19.1, 5000.0, 3.3, 0.0]) -> \n",
      "      (global, 5000.0)\n",
      "  Applying f_reduce to reduce_input partition 1\n"
     ]
    }
   ],
   "source": [
    "result = mapreduce(partitioned_sensor_measurements, *a2_t1b_global_max_measurement(), print_debug_text=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04769256",
   "metadata": {},
   "source": [
    "Test your program by running the following cell, which checks whether your program produces the correct output for our example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e766cf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT VERIFICATION:\n",
      "  PASS: Key [global] has expected value!\n"
     ]
    }
   ],
   "source": [
    "verify_locally(result, 1, [(\"global\", 5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "987211b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local tests...\n",
      "a2_t1b_global_max_measurement successfully passed local tests\n",
      "Running remote test...\n",
      "Test was successful. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "am.test_student_function(a2_t1b_global_max_measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231b9b3",
   "metadata": {},
   "source": [
    "## Task 2 - Text Processing\n",
    "\n",
    "In the next task, we implement some simple text processing program with mapreduce. Our input data is partitioned data about books, where each key value pair contains a __book_id__ as key and a __sentence__ as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "738dfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_books = [\n",
    "    [\n",
    "        (1, \"It was a bright cold day in April, and the clocks were striking thirteen.\"),\n",
    "        (2, \"Mr. Jones, of the Manor Farm, had locked the hen-houses for the night, but was too drunk to remember to shut the pop-holes. \"),\n",
    "        (3, \"He was a tough-looking youth of twenty-five or six, with reddish-yellow hair and powerful shoulders.\")\n",
    "    ],\n",
    "    [\n",
    "        (1, \"On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall.\"),\n",
    "        (2, \"All the animals were now present except Moses, the tame raven, who slept on a perch behind the back door.\"),        \n",
    "        (3, \"This was in late December 1936, less than seven months ago as I write, and yet it is a period that has already receded into enormous distance.\")\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae67a9",
   "metadata": {},
   "source": [
    "### Compute the number of commas per book.\n",
    "\n",
    "Please write a program to count the number of commas occurring in the text of each book. Implement your code in the `commacount_f_map` and `commacount_f_reduce` functions (and change the return statement).\n",
    "\n",
    "Please note that the result of your mapreduce program should be equivalent to running the following SQL query on the data (if it was in a database and there was a CHAR_COUNT function):\n",
    "\n",
    "```\n",
    "SELECT book_id, SUM(CHAR_COUNT(sentence, ','))\n",
    "FROM book_sentences\n",
    "GROUP BY book_id\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "1acd0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def a2_t2_commacount():\n",
    "    def commacount_f_map(book_id, sentence):\n",
    "        output_tuples = []\n",
    "        for word in re.findall(r\"[\\w']+|[.,!?;]\", sentence):\n",
    "            if word == ',':\n",
    "                output_tuples.append((book_id, 1))\n",
    "        return output_tuples         \n",
    "\n",
    "\n",
    "    def commacount_f_reduce(book_id, counts):\n",
    "        return [(book_id, sum(counts))]\n",
    "\n",
    "    return commacount_f_map, commacount_f_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "525904ec-ca09-46b9-8e36-c8c666f3763e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IKF', ',', ',', ',', ',', ',', 'IWr']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[\\w']+|[.,!?;]\", 'IKF,,,,,IWr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "097aac81-8e15-44d4-b091-1a2fd0db8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def a2_t2_commacount():\n",
    "    def commacount_f_map(book_id, sentence):\n",
    "        return [(book_id, sentence.count(','))]         \n",
    "\n",
    "\n",
    "    def commacount_f_reduce(book_id, counts):\n",
    "        total_count = 0\n",
    "        for count in counts:\n",
    "            total_count += count\n",
    "        return [(book_id, total_count)]\n",
    "\n",
    "    return commacount_f_map, commacount_f_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd3a27",
   "metadata": {},
   "source": [
    "Execute your program by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "77348911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting MAP-phase.---\n",
      "  Applying f_map to input partition 0\n",
      "    f_map(1, It was a bright cold day in April, and the clocks were striking thirteen.) -> \n",
      "      (1, 1)\n",
      "    f_map(2, Mr. Jones, of the Manor Farm, had locked the hen-houses for the night, but was too drunk to remember to shut the pop-holes. ) -> \n",
      "      (2, 3)\n",
      "    f_map(3, He was a tough-looking youth of twenty-five or six, with reddish-yellow hair and powerful shoulders.) -> \n",
      "      (3, 1)\n",
      "  Applying f_map to input partition 1\n",
      "    f_map(1, On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall.) -> \n",
      "      (1, 2)\n",
      "    f_map(2, All the animals were now present except Moses, the tame raven, who slept on a perch behind the back door.) -> \n",
      "      (2, 2)\n",
      "    f_map(3, This was in late December 1936, less than seven months ago as I write, and yet it is a period that has already receded into enormous distance.) -> \n",
      "      (3, 2)\n",
      "\n",
      "---Starting SHUFFLE-phase.---\n",
      "  Shuffling map output input partition 0\n",
      "    Assigning key [1] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [2] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [3] to reducer input 1 via hash-partitioning\n",
      "  Shuffling map output input partition 1\n",
      "    Assigning key [1] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [2] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [3] to reducer input 1 via hash-partitioning\n",
      "\n",
      "---Starting REDUCE-phase.---\n",
      "  Applying f_reduce to reduce_input partition 0\n",
      "    f_reduce(2, [3, 2]) -> \n",
      "      (2, 5)\n",
      "  Applying f_reduce to reduce_input partition 1\n",
      "    f_reduce(1, [1, 2]) -> \n",
      "      (1, 3)\n",
      "    f_reduce(3, [1, 2]) -> \n",
      "      (3, 3)\n"
     ]
    }
   ],
   "source": [
    "result = mapreduce(partitioned_books, *a2_t2_commacount(), print_debug_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c803aa",
   "metadata": {},
   "source": [
    "Test your program by running the following cell, which checks whether your program produces the correct output for our example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "d3bbb671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT VERIFICATION:\n",
      "  PASS: Key [2] has expected value!\n",
      "  PASS: Key [1] has expected value!\n",
      "  PASS: Key [3] has expected value!\n"
     ]
    }
   ],
   "source": [
    "verify_locally(result, 3, [(2, 5), (1, 3), (3, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "f1a55bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local tests...\n",
      "a2_t2_commacount successfully passed local tests\n",
      "Running remote test...\n",
      "Test was successful. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "am.test_student_function(a2_t2_commacount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65083fe",
   "metadata": {},
   "source": [
    "### Task 3 - Movie visitor numbers\n",
    "\n",
    "In the final mapreduce task of this assignment, we work with key value pairs about the visitor numbers of a movie for several weeks. Note that the data is a bit more messy than before. The key denotes the __name__ of the movie, but the value is a nested array of tuples, denoting the number of the __week__ and the __number of visitors__ in that week for a given movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "0bc41aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_movie_stats = [\n",
    "    [\n",
    "        (\"MovieA\", [(1, 100), (2, 20), (3, \"50\")]),\n",
    "        (\"MovieC\", [(1, 100), (2, \"250\"), (3, 100), (4, \"120\")]),        \n",
    "        (\"MovieB\", [(1, 1000), (2, 250)]),\n",
    "\n",
    "    ],\n",
    "    [\n",
    "        (\"MovieA\", [(4, 50), (5, \"10\"), (6, 0)]),\n",
    "        (\"MovieB\", [(3, 0), (4, \"260\")]),  \n",
    "        (\"MovieC\", [(5, \"180\")]),\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9b609",
   "metadata": {},
   "source": [
    "### Maximum number of visitors in a week per movie\n",
    "\n",
    "Please write a mapreduce program to compute the maximum number of visitors in a week per movie (but ignore the first week of each movie). Implement your code in the `max_weekly_visitors_f_map` and `max_weekly_visitors_f_reduce` functions (and change the return statement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "d14d24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2_t3_max_weekly_visitors():\n",
    "    def max_weekly_visitors_f_map(movie, weekly_stats):   \n",
    "        l = []\n",
    "        for i in weekly_stats:\n",
    "            if i[0] != 1:\n",
    "                l.append(int(i[1]))\n",
    "        return [(movie, max(l))]\n",
    "\n",
    "    def max_weekly_visitors_f_reduce(movie, numbers):\n",
    "        return [(movie, max(numbers))]\n",
    "    \n",
    "    return max_weekly_visitors_f_map, max_weekly_visitors_f_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe40325",
   "metadata": {},
   "source": [
    "Execute your program by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "32fec880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Starting MAP-phase.---\n",
      "  Applying f_map to input partition 0\n",
      "    f_map(MovieA, [(1, 100), (2, 20), (3, '50')]) -> \n",
      "      (MovieA, 50)\n",
      "    f_map(MovieC, [(1, 100), (2, '250'), (3, 100), (4, '120')]) -> \n",
      "      (MovieC, 250)\n",
      "    f_map(MovieB, [(1, 1000), (2, 250)]) -> \n",
      "      (MovieB, 250)\n",
      "  Applying f_map to input partition 1\n",
      "    f_map(MovieA, [(4, 50), (5, '10'), (6, 0)]) -> \n",
      "      (MovieA, 50)\n",
      "    f_map(MovieB, [(3, 0), (4, '260')]) -> \n",
      "      (MovieB, 260)\n",
      "    f_map(MovieC, [(5, '180')]) -> \n",
      "      (MovieC, 180)\n",
      "\n",
      "---Starting SHUFFLE-phase.---\n",
      "  Shuffling map output input partition 0\n",
      "    Assigning key [MovieA] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [MovieC] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [MovieB] to reducer input 1 via hash-partitioning\n",
      "  Shuffling map output input partition 1\n",
      "    Assigning key [MovieA] to reducer input 0 via hash-partitioning\n",
      "    Assigning key [MovieB] to reducer input 1 via hash-partitioning\n",
      "    Assigning key [MovieC] to reducer input 0 via hash-partitioning\n",
      "\n",
      "---Starting REDUCE-phase.---\n",
      "  Applying f_reduce to reduce_input partition 0\n",
      "    f_reduce(MovieA, [50, 50]) -> \n",
      "      (MovieA, 50)\n",
      "    f_reduce(MovieC, [250, 180]) -> \n",
      "      (MovieC, 250)\n",
      "  Applying f_reduce to reduce_input partition 1\n",
      "    f_reduce(MovieB, [250, 260]) -> \n",
      "      (MovieB, 260)\n"
     ]
    }
   ],
   "source": [
    "result = mapreduce(partitioned_movie_stats, *a2_t3_max_weekly_visitors(), print_debug_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a300b",
   "metadata": {},
   "source": [
    "Test your program by running the following cell, which checks whether your program produces the correct output for our example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "112520ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT VERIFICATION:\n",
      "  PASS: Key [MovieA] has expected value!\n",
      "  PASS: Key [MovieB] has expected value!\n",
      "  PASS: Key [MovieC] has expected value!\n"
     ]
    }
   ],
   "source": [
    "verify_locally(result, 3, [('MovieA', 50), ('MovieB', 260), ('MovieC', 250)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "013bed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running local tests...\n",
      "a2_t3_max_weekly_visitors successfully passed local tests\n",
      "Running remote test...\n",
      "Test was successful. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "am.test_student_function(a2_t3_max_weekly_visitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f239dc",
   "metadata": {},
   "source": [
    "# Part B - Spark\n",
    "\n",
    "In the second part of the assignment, we revisit our problems but solve them with a real dataflow system now: [Apache Spark](https://spark.apache.org/docs/latest/api/python/index.html). Please note that we use its Python variant in local mode for simplicity and easy debugging. But your programs would also run on a real cluster!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e5957",
   "metadata": {},
   "source": [
    "But first, we define other helper function to test our program outputs locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "9f012078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_locally_from_rdd(result, expected_num_keys, expected_keys_and_values):\n",
    "    verify_locally([result.collect()], expected_num_keys, expected_keys_and_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544a7343",
   "metadata": {},
   "source": [
    "Spark has the concept of a session that allows us to move data into the cluster. We use it in local mode though.\n",
    "\n",
    "Note that you can ignore the following warnings that might occur when executing the next cell:\n",
    "\n",
    "```\n",
    "WARNING: An illegal reflective access operation has occurred\n",
    "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
    "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
    "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
    "WARNING: All illegal access operations will be denied in a future release\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```\n",
    " WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "82a199ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/23 15:30:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"127.0.0.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e9f7c",
   "metadata": {},
   "source": [
    "Spark works on so-called RDDS (resilient distributed datasets). We revisit our \"hello world\" example and create an RDD of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "951c8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = spark.sparkContext.parallelize([\n",
    "    (1, \"the shawshank redemption\"),\n",
    "    (2, \"the godfather\"),\n",
    "    (3, \"the godfather part two\")\n",
    "], 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "7aaf2bc0-1abe-4534-99df-c410ae74ed9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[19] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae7df7",
   "metadata": {},
   "source": [
    "We write programs in Spark by RDDs into new RDDs. Similar to MapReduce we can write our own functions to apply to the data during these transformations. \n",
    "\n",
    "Be sure to consult the [overview over the PySpark RDD API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis) for the following tasks.\n",
    "\n",
    "\n",
    "The following code shows our \"Hello World\" example in Spark. We apply a `flatMap` to transform an document into multiple word tuples (analogously to our f_map function in mapreduce) and use Spark's `reduceByKey` transformation to group and reduce the resulting output. Note that we only have to define how to aggregate (e.g., sum) the values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6aa350a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(docid_and_title):\n",
    "    text = docid_and_title[1] # must be a tuplet\n",
    "    output_tuples = []\n",
    "    for word in text.split(\" \"):\n",
    "        output_tuples.append((word, 1))\n",
    "    return output_tuples # list of tuplets       \n",
    "\n",
    "wordcounts = documents\\\n",
    "    .flatMap(lambda docid_and_title: extract_words(docid_and_title))\\\n",
    "    .reduceByKey(lambda count1, count2: count1 + count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411509d",
   "metadata": {},
   "source": [
    "We can use the adapted test function to verify that the outputs of our program are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "d02cdc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT VERIFICATION:\n",
      "  PASS: Key [the] has expected value!\n",
      "  PASS: Key [godfather] has expected value!\n",
      "  PASS: Key [two] has expected value!\n"
     ]
    }
   ],
   "source": [
    "verify_locally_from_rdd(wordcounts, 6, [(\"the\", 3), (\"godfather\", 2), (\"two\", 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e476278",
   "metadata": {},
   "source": [
    "Spark RDD's have a built-in action `collect` which transforms them into a Python collection. In real-world usecases, this would load the data from the cluster into the memory of our program (and potentially cause a crash), but we can use it here for debugging and inspecting the outputs of our transformations.\n",
    "\n",
    "Here is how the data looks after the flatMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "c7c2a94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1),\n",
       " ('shawshank', 1),\n",
       " ('redemption', 1),\n",
       " ('the', 1),\n",
       " ('godfather', 1),\n",
       " ('the', 1),\n",
       " ('godfather', 1),\n",
       " ('part', 1),\n",
       " ('two', 1)]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents\\\n",
    "    .flatMap(lambda docid_and_title: extract_words(docid_and_title))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572c1f8",
   "metadata": {},
   "source": [
    "And this cell shows how the data looks like after the reduce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "ce181f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('godfather', 2),\n",
       " ('two', 1),\n",
       " ('the', 3),\n",
       " ('shawshank', 1),\n",
       " ('redemption', 1),\n",
       " ('part', 1)]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents\\\n",
    "    .flatMap(lambda docid_and_title: extract_words(docid_and_title))\\\n",
    "    .reduceByKey(lambda count1, count2: count1 + count2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d756db8",
   "metadata": {},
   "source": [
    "Note that we can often write Spark programs without having to explicitly define new functions, we can also use lambda functions and list comprehensions, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "f43640d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = documents\\\n",
    "    .flatMap(lambda docid_and_title: [(word, 1) for word in docid_and_title[1].split(\" \")])\\\n",
    "    .reduceByKey(lambda count1, count2: count1 + count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "c2718933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULT VERIFICATION:\n",
      "  PASS: Key [the] has expected value!\n",
      "  PASS: Key [godfather] has expected value!\n",
      "  PASS: Key [two] has expected value!\n"
     ]
    }
   ],
   "source": [
    "verify_locally_from_rdd(wordcounts, 6, [(\"the\", 3), (\"godfather\", 2), (\"two\", 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131b7cb",
   "metadata": {},
   "source": [
    "### Task 4 - Sensors again\n",
    "\n",
    "Next, we revisit our sensors task. First we load the measurement data into an RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "079f2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_measurements = spark.sparkContext.parallelize([\n",
    "        (\"Sensor1\", 12.3),\n",
    "        (\"Sensor2\", 9.3),\n",
    "        (\"Sensor3\", 2.7),\n",
    "        (\"BrokenSensor\", 1000.0),\n",
    "        (\"Sensor1\", 17.1),\n",
    "        (\"Sensor2\", 7.3),\n",
    "        (\"Sensor3\", -12),\n",
    "        (\"Sensor1\", 19.1),\n",
    "        (\"BrokenSensor\", 5000.0),\n",
    "        (\"Sensor2\", 3.3),\n",
    "        (\"Sensor3\", 0.0)\n",
    "], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8ca01efb-ccf1-4d58-b6a2-d285d5d06e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[21] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474be3c-2b69-4039-9543-ace3ec176f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def max_measurement_per_sensor_f_map(sensor, measurement): \n",
    "        output = []\n",
    "        if sensor != \"BrokenSensor\":\n",
    "            output.append((sensor, measurement))\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def max_measurement_per_sensor_f_reduce(sensor, measurements):\n",
    "        out = [(sensor, max(measurements))]\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be409884",
   "metadata": {},
   "source": [
    "### Task 4(a) - Maximum value per sensor\n",
    "\n",
    "Your task is to compute an RDD containing the maximum measurement value per sensor, for all sensors except the \"BrokenSensor\". Write the corresponding code in the `compute_max_value_per_sensor` function (and change the return statement).\n",
    "\n",
    "Note that you can use the next cell for locally trying out some Spark code, before you fill in your solution into the `compute_max_value_per_sensor` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "96d9679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for local development, you can test different transformations here\n",
    "\n",
    "def a2_t4a_compute_max_value_per_sensor(sensor_measurements):\n",
    "    print(sensor_measurements)\n",
    "    sensor = sensor_measurements[0]\n",
    "    measurement = sensor_measurements[1]\n",
    "    output = []\n",
    "    if sensor != \"BrokenSensor\":\n",
    "        output.append((sensor, max(measurement)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "10e4db03-c36c-4d3a-8b01-dc1b5a97a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Sensor1', 12.3)\n",
      "22/02/23 16:23:06 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 16)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_55/3564516574.py\", line 2, in <lambda>\n",
      "  File \"/tmp/ipykernel_55/1278843982.py\", line 9, in a2_t4a_compute_max_value_per_sensor\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/02/23 16:23:06 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 16) (e52e45bb1fdc executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_55/3564516574.py\", line 2, in <lambda>\n",
      "  File \"/tmp/ipykernel_55/1278843982.py\", line 9, in a2_t4a_compute_max_value_per_sensor\n",
      "TypeError: 'float' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/02/23 16:23:06 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job\n",
      "22/02/23 16:23:06 WARN TaskSetManager: Lost task 1.0 in stage 9.0 (TID 17) (e52e45bb1fdc executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 16) (e52e45bb1fdc executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_55/3564516574.py\", line 2, in <lambda>\n  File \"/tmp/ipykernel_55/1278843982.py\", line 9, in a2_t4a_compute_max_value_per_sensor\nTypeError: 'float' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_55/3564516574.py\", line 2, in <lambda>\n  File \"/tmp/ipykernel_55/1278843982.py\", line 9, in a2_t4a_compute_max_value_per_sensor\nTypeError: 'float' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [367]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msensor_measurements\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msensor_measurements\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma2_t4a_compute_max_value_per_sensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msensor_measurements\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 950\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 16) (e52e45bb1fdc executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_55/3564516574.py\", line 2, in <lambda>\n  File \"/tmp/ipykernel_55/1278843982.py\", line 9, in a2_t4a_compute_max_value_per_sensor\nTypeError: 'float' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 611, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_55/3564516574.py\", line 2, in <lambda>\n  File \"/tmp/ipykernel_55/1278843982.py\", line 9, in a2_t4a_compute_max_value_per_sensor\nTypeError: 'float' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "sensor_measurements\\\n",
    "    .flatMap(lambda sensor_measurements: a2_t4a_compute_max_value_per_sensor(sensor_measurements))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "3ae7b1dc-af0a-405a-8045-8d4eb4207f94",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (3189857397.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [365]\u001b[0;36m\u001b[0m\n\u001b[0;31m    .reduceByKey(lambda count1, count2: count1 + count2)\u001b[0m\n\u001b[0m                                                        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sensor_measurements\\\n",
    "    .flatMap((lambda sensor_measurements: a2_t4a_compute_max_value_per_sensor(sensor_measurements))\\\n",
    "    .reduceByKey(lambda count1, count2: count1 + count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "1ad75423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2_t4a_compute_max_value_per_sensor(sensor_measurements):\n",
    "    sensor = sensor_measurements[0]\n",
    "    measurement = sensor_measurements[1]\n",
    "    output = []\n",
    "    if sensor != \"BrokenSensor\":\n",
    "        return [(sensor, measurement)]\n",
    "    #return output\n",
    "\n",
    "\n",
    "    #return sensor_measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422ca50",
   "metadata": {},
   "source": [
    "Execute the following cell to test your Spark program locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f5b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a2_t4a_compute_max_value_per_sensor(sensor_measurements)\n",
    "\n",
    "verify_locally_from_rdd(result, 3, [(\"Sensor1\", 19.1), (\"Sensor2\", 9.3), (\"Sensor3\", 2.7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(a2_t4a_compute_max_value_per_sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6bb06",
   "metadata": {},
   "source": [
    "### Task 4(b) - Globally maximal sensor value\n",
    "\n",
    "In the next task, you have to compute an RDD with the globally maximal measurement value from all sensors (including the \"BrokenSensor\"). Implement your code in the `compute_global_max_value` function (and change the return statement). Hint: use an artificial key \"global\" to produce the final output.\n",
    "\n",
    "Note that you can use the next cell for locally trying out some Spark code, before you fill in your solution into the `compute_global_max_value` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for local development, you can test different transformations here\n",
    "sensor_measurements\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f51620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2_t4b_compute_global_max_value(sensor_measurements):\n",
    "    # IMPLEMENT ME\n",
    "    return sensor_measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445aff7",
   "metadata": {},
   "source": [
    "Execute the following cell to test your Spark program locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79012fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a2_t4b_compute_global_max_value(sensor_measurements)\n",
    "verify_locally_from_rdd(result, 1, [(\"global\", 5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f431f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(a2_t4b_compute_global_max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfc98ef",
   "metadata": {},
   "source": [
    "### Task 5 - Movie stats again\n",
    "\n",
    "Next, we also revisit our movie stats task from before. First, we turn the data into an RDD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db134ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_stats = spark.sparkContext.parallelize([\n",
    "    (\"MovieA\", [(1, 100), (2, 20), (3, \"50\")]),\n",
    "    (\"MovieC\", [(1, 100), (2, \"250\"), (3, 100), (4, \"120\")]),        \n",
    "    (\"MovieB\", [(1, 1000), (2, 250)]),\n",
    "    (\"MovieA\", [(4, 50), (5, \"10\"), (6, 0)]),\n",
    "    (\"MovieB\", [(3, 0), (4, \"260\")]),  \n",
    "    (\"MovieC\", [(5, \"180\")]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f6439",
   "metadata": {},
   "source": [
    "Please write a Spark program to compute an RDD containing the maximum amount of visitors per week per movie (but ignore the first week of each movie). Implement your code in the `compute_max_weekly_visitors` function (and change the return statement).\n",
    "\n",
    "Note that you can use the next cell for locally trying out some Spark code, before you fill in your solution into the `compute_max_weekly_visitors` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefe3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for local development, you can test different transformations here\n",
    "movie_stats\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2822fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2_t5_compute_max_weekly_visitors(movie_stats):\n",
    "    # IMPLEMENT ME    \n",
    "    return movie_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08677e99",
   "metadata": {},
   "source": [
    "Execute the following cell to test your Spark program locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d22185",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a2_t5_compute_max_weekly_visitors(movie_stats)\n",
    "verify_locally_from_rdd(result, 3, [('MovieA', 50), ('MovieB', 260), ('MovieC', 250)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(a2_t5_compute_max_weekly_visitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb23c0a1",
   "metadata": {},
   "source": [
    "# Part C - Spark SQL\n",
    "\n",
    "In the final part of this assignment, we explore how Spark unifies relational processing and dataflow computation via [SparkSQL](https://spark.apache.org/docs/3.2.1/sql-programming-guide.html). Spark has a distributed version of the well-known dataframes and allows us to query them with SQL.\n",
    "\n",
    "Be sure to consult the [overview over PySpark's dataframe API](https://spark.apache.org/docs/3.2.1/api/python/reference/pyspark.sql.html#dataframe-apis) for the following tasks.\n",
    "\n",
    "\n",
    "Let us first define two dataframes. They might remind you of the tables in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf3eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "customers = spark.sparkContext.parallelize([\n",
    "    Row(customer_id='1122334455', firstname='Ann', lastname='O\\'Brien', city='Rotterdam'),\n",
    "    Row(customer_id='1231231231', firstname='John', lastname='Doe', city='Amsterdam'), \n",
    "    Row(customer_id='1234567890', firstname='Paul', lastname='Murphy', city='Diemen'), \n",
    "    Row(customer_id='9876543210', firstname='Jack', lastname='Murphy', city='Utrecht'), \n",
    "    Row(customer_id='9999999999', firstname='Norah', lastname='Jones', city='Amsterdam')\n",
    "], 2).toDF()  \n",
    "\n",
    "sales = spark.sparkContext.parallelize([\n",
    "    Row(customer_id='1122334455', model='2010', quantity=1, paid=2300, type_of_payment='mastercard credit'), \n",
    "    Row(customer_id='1122334455', model='3001', quantity=1, paid=99, type_of_payment='cash'), \n",
    "    Row(customer_id='1231231231', model='2002', quantity=2, paid=1898, type_of_payment='visa credit'), \n",
    "    Row(customer_id='1231231231', model='3002', quantity=1, paid=239, type_of_payment='cash'), \n",
    "    Row(customer_id='1234567890', model='1001', quantity=1, paid=1902, type_of_payment='mastercard credit'), \n",
    "    Row(customer_id='9876543210', model='1007', quantity=1, paid=510, type_of_payment='visa debit'), \n",
    "    Row(customer_id='9876543210', model='1007', quantity=3, paid=1530, type_of_payment='visa debit'), \n",
    "    Row(customer_id='9876543210', model='2002', quantity=1, paid=949, type_of_payment='visa debit'), \n",
    "    Row(customer_id='9999999999', model='1007', quantity=1, paid=459, type_of_payment='visa credit'), \n",
    "    Row(customer_id='9999999999', model='3007', quantity=2, paid=360, type_of_payment='visa credit')\n",
    "], 2).toDF()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa20ef",
   "metadata": {},
   "source": [
    "Spark dataframes have a built-in method `show` which allows us to take a look at the first rows of the dataframe. We can use this to inspect our sales and customer data. It looks a lot like relational tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c12099",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cf017",
   "metadata": {},
   "source": [
    "Now we can use the SparkSQL API to transform these dataframes with SQL. As an example, let's assume we are interested in the number of customers per city. In a relational database, we could query for this quantity like this:\n",
    "\n",
    "```\n",
    "SELECT city, COUNT(*)\n",
    "FROM customers\n",
    "GROUP BY city\n",
    "```\n",
    "\n",
    "Spark allows us to write a dataflow program that produces identical results to the query and also looks very much a like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_with_counts = customers\\\n",
    "  .groupby(\"city\")\\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8072745",
   "metadata": {},
   "source": [
    "We can again use `show` to inspect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c8e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_with_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f964d16d",
   "metadata": {},
   "source": [
    "Let's look at another example, what are the distinct types of payment in the sales data? In SQL, a corresponding query would look like this:\n",
    "\n",
    "```\n",
    "SELECT DISTINCT type_of_payment\n",
    "FROM sales\n",
    "```\n",
    "\n",
    "Again, the corresponding Spark code is very similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_types = sales\\\n",
    "    .select(sales['type_of_payment'])\\\n",
    "    .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_types.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050eede",
   "metadata": {},
   "source": [
    "Before we give you more tasks, we first define another helper function to verify SparkSQL results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141af937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_sql_result_locally(observed_dataframe, expected_rows):\n",
    "    observed_rows = observed_dataframe.collect()\n",
    "    if len(observed_rows) != len(expected_rows):\n",
    "        print(f\"ERROR: Expected {len(expected_rows)} rows in the result, but found {len(observed_rows)}!\")\n",
    "    \n",
    "    for expected_row in expected_rows:\n",
    "        if not expected_row in observed_rows:\n",
    "            print(f\"ERROR: Row {expected_row} missing from result!\")\n",
    "        else:    \n",
    "            print(f\"PASS: Encountered expected row {expected_row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbf045",
   "metadata": {},
   "source": [
    "### Task 6 - Filter customers\n",
    "\n",
    "Write a SparkSQL program to find the distinct ids of all customers who made a visa credit payment of less than 1000 euros. Implement the `customers_with_certain_visa_payment` function, and make it return the correct Dataframe.\n",
    "\n",
    "Note that you can use the next cell for locally trying out some Spark code, before you fill in your solution into the `customers_with_certain_visa_payment` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2598ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for local development, you can test different transformations here\n",
    "sales\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1374a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2_t6_customers_with_certain_visa_payment(sales):\n",
    "    # IMPLEMENT ME    \n",
    "    return sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebe969",
   "metadata": {},
   "source": [
    "Execute the following cell to test your Spark SQL program locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72fc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a2_t6_customers_with_certain_visa_payment(sales)\n",
    "verify_sql_result_locally(result, [Row(customer_id='9999999999')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(a2_t6_customers_with_certain_visa_payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6457b1",
   "metadata": {},
   "source": [
    "### Task 7 - Payment types in Amsterdam\n",
    "\n",
    "Write a SparkSQL program to find the payment types used by customers in a given city. No type should occur more than once. Implement the `find_payment_types` function to return the correct Dataframe. You can use Spark's `join` function to join the two tables, you can find the documentation [here](https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.sql.DataFrame.join.html?highlight=join#pyspark.sql.DataFrame.join).\n",
    "\n",
    "Note that you can use the next cell for locally trying out some Spark code, before you fill in your solution into the `find_payment_types` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71b2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for local development, you can test different transformations here\n",
    "sales\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e0328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2_t7_find_payment_types(sales, city, customers):\n",
    "    # IMPLEMENT ME\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250bf05d",
   "metadata": {},
   "source": [
    "Execute the following cell to test your Spark SQL program locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81932945",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a2_t7_find_payment_types(sales, \"Amsterdam\", customers)\n",
    "verify_sql_result_locally(result, [Row(type_of_payment='visa credit'), Row(type_of_payment='cash')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4954e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(a2_t7_find_payment_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c486164f",
   "metadata": {},
   "source": [
    "### Task 8 - Number of sales and money per city\n",
    "\n",
    "Write a spark SQL program that computes the amount of money paid per city.  Implement the `quantities_per_city` function to return the correct Dataframe with the columns __city__ and __amount__. To find the aggregation function you need for this, you can look at the `.groupby()` [documentation](https://spark.apache.org/docs/3.2.1/api/python/reference/api/pyspark.sql.DataFrame.groupBy.html). Depending on how you do this, you may want to use Spark's `withColumnRenamed` function to rename columns where appropriate.\n",
    "\n",
    "Note that you can use the next cell for locally trying out some Spark code, before you fill in your solution into the `quantities_per_city` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf294e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for local development, you can test different transformations here\n",
    "sales\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2_t8_quantities_per_city(sales, customers):\n",
    "    # IMPLEMENT ME\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b317035d",
   "metadata": {},
   "source": [
    "Execute the following cell to test your Spark SQL program locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc647bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = a2_t8_quantities_per_city(sales, customers)\n",
    "\n",
    "expected_rows = [\n",
    "    Row(city='Diemen', amount=1902),\n",
    "    Row(city='Rotterdam', amount=2399),\n",
    "    Row(city='Amsterdam', amount=2956),\n",
    "     Row(city='Utrecht', amount=2989)\n",
    "]\n",
    "\n",
    "verify_sql_result_locally(result, expected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "am.test_student_function(a2_t8_quantities_per_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ef58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
