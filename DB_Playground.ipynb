{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'imdb/data/csv/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-548decabae80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;31m# with open('mapper.json', 'r', encoding='') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-548decabae80>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.compart.com/en/unicode\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_special_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# self.load_json_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-548decabae80>\u001b[0m in \u001b[0;36mload_train_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{DataManager.imdb_data_folder}/csv/train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtraining_data_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imdb/data/csv/train'"
     ]
    }
   ],
   "source": [
    "class DataManager():\n",
    "    imdb_data_folder = 'imdb/data'\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.url = \"https://www.compart.com/en/unicode\"\n",
    "        self.mapper = {}\n",
    "        self.load_train_data()\n",
    "        self.extract_special_chars()\n",
    "        # self.load_json_data()\n",
    "        # self.load_validation_data()\n",
    "        # self.load_test_data()\n",
    "\n",
    "    def load_train_data(self):\n",
    "        path = f\"{DataManager.imdb_data_folder}/csv/train\" \n",
    "        training_data_files = os.listdir(path=path)\n",
    "\n",
    "        train_df = pd.DataFrame()\n",
    "        for file in training_data_files:\n",
    "            if '0' in file:\n",
    "                continue\n",
    "            df = pd.read_csv(f\"{path}/{file}\")\n",
    "            train_df = pd.concat([train_df, df], ignore_index=True)\n",
    "            \n",
    "        self.train_df = train_df.rename(columns={\"Unnamed: 0\": 'index'}).sort_values(by='index').set_index('index').fillna('NULL')\n",
    "        self.train_df.to_csv(f\"{path}/train-0.csv\", sep=';')\n",
    "    \n",
    "    \n",
    "    def load_validation_data(self):\n",
    "        path = f\"{DataManager.imdb_data_folder}/csv/test_and_validation\" \n",
    "        df = pd.read_csv(f\"{path}/validation_hidden.csv\")\n",
    "        self.validation_df = df.rename(columns={\"Unnamed: 0\": 'index'}).sort_values(by='index').set_index('index')\n",
    "\n",
    "    def load_test_data(self):\n",
    "        path = f\"{DataManager.imdb_data_folder}/csv/test_and_validation\" \n",
    "        df = pd.read_csv(f\"{path}/test_hidden.csv\")\n",
    "        self.test_df = df.rename(columns={\"Unnamed: 0\": 'index'}).sort_values(by='index').set_index('index')\n",
    "\n",
    "    def load_json_data(self):\n",
    "        path = f\"{DataManager.imdb_data_folder}/json\"\n",
    "        \n",
    "        self.directing_df = pd.read_json(f\"{path}/directing.json\")\n",
    "        self.writing_df = pd.read_json(f\"{path}/writing.json\")\n",
    "\n",
    "        self.joined_df = pd.merge(self.writing_df, self.directing_df, how='left', on='movie')\n",
    "\n",
    "    def extract_special_chars(self):\n",
    "        pattern = r'[a-zA-Z0-9 ,Â°!?@#$%&:;+~_/\\-\\\"\\'\\^\\*\\(\\)\\.\\[\\]]'\n",
    "        self.train_df['special_chars'] = self.train_df['primaryTitle'].apply(lambda x: re.sub(pattern, '', x))\n",
    "        special_values_df = self.train_df[self.train_df['special_chars'] != ''][['special_chars']]\n",
    "        self.index = special_values_df.index.tolist() \n",
    "\n",
    "        for chars in special_values_df['special_chars'].tolist():\n",
    "            for char in chars:\n",
    "                if char in self.mapper:\n",
    "                    continue\n",
    "                self.add_char_to_mapper(char)\n",
    "        \n",
    "        with open('mapper.json', 'w') as f:\n",
    "            json.dump(self.mapper, f, indent=2)\n",
    "\n",
    "    def add_char_to_mapper(self, char):\n",
    "        hex_value = hex(ord(char))[2:]\n",
    "        unicode = f\"U+{hex_value.zfill(4).upper()}\"\n",
    "\n",
    "        response = requests.get(f\"{self.url}/{unicode}\")\n",
    "        soup = BeautifulSoup(response.text, 'html.parser').table.find(\"tbody\")\n",
    "\n",
    "        rows = soup.find_all(\"tr\")\n",
    "        nr_rows = len(rows)\n",
    "        for i, row in enumerate(rows):\n",
    "            if nr_rows != i + 1:\n",
    "                continue\n",
    "            last_row = row\n",
    "        td = last_row.find('td', {\"class\": 'second-column'})\n",
    "        val = td.findChild().text.split(' ')[0]\n",
    "        if re.match('[a-zA-Z]', val): \n",
    "            self.mapper[char] = val\n",
    "\n",
    "    def restore_column(self):\n",
    "        mask = self.train_df.index.isin(self.index)\n",
    "        self.train_df.loc[mask, 'primaryTitle'] = self.train_df[mask]['primaryTitle'].apply(lambda x: self.replace_chars(x))\n",
    "\n",
    "    def replace_chars(self, s):\n",
    "        print(s)\n",
    "        for char in self.mapper.keys():\n",
    "            if char in s:\n",
    "                s = s.replace(char, self.mapper[char]) \n",
    "        print(s)\n",
    "        return s\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    m = DataManager()\n",
    "    m.restore_column()\n",
    "    # with open('mapper.json', 'r', encoding='') as f:\n",
    "    #     mapper = json.loads(f.read())\n",
    "    # print(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data_folder = 'imdb/data'\n",
    "\n",
    "path = f\"{imdb_data_folder}/csv/train\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb/data/csv/train\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'imdb/data/csv/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-131d6aecd5a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imdb/data/csv/train'"
     ]
    }
   ],
   "source": [
    "training_data_files = os.listdir(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.Rhistory',\n",
       " 'Test_Folder',\n",
       " '.DS_Store',\n",
       " '.localized',\n",
       " 'Screenshot 2022-03-04 at 18.48.31.png',\n",
       " 'Assignment 3 Big Data.pdf',\n",
       " 'Causal Data Science Sample Paper-1',\n",
       " 'barplot 09.52.39.png',\n",
       " 'KeepInMind',\n",
       " '~$bewijs.txt',\n",
       " 'heatmap.png',\n",
       " '~$ogress_DJ.docx']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('/Users/dj/Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
